---
title: "Data Processing — OFPE Analysis-Ready Build"
subtitle: "Soils, Topography, and Timing-Specific Weather"
author: "(pipeline orchestration)"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
execute:
  echo: true
  message: true
  warning: true
---

```{r setup}
#| label: setup
#| include: false
library(here)
knitr::opts_chunk$set(message = TRUE, warning = TRUE)
source("Code/src/functions_for_processing.R")
```

## 1. Configuration & Inputs

```{r config}
# Manually define parameters here (previously in YAML params)
base_dir <- here::here("Data","Raw")
out_dir  <- here::here("Data","Processed","Analysis_ready")
elev_z   <- 14
crs_out  <- 4326
ffy_ids  <- NULL  # Optional subset of field-year IDs; if NULL, auto-discover

# Date manifest: must contain ffy_id, s_time, n_time, yield_time
# Adjust path to your actual manifest file (RDS or CSV). Here we assume RDS.
date_manifest_path <- here::here("Data","Raw","date_product_info.rds")
if (!file.exists(date_manifest_path)) stop("Missing date manifest at ", date_manifest_path)

date_manifest <- readRDS(date_manifest_path) |>
  dplyr::select(ffy_id, s_time, n_time, yield_time) |>
  dplyr::mutate(
    s_time     = as.Date(s_time),
    n_time     = as.Date(n_time),
    yield_time = as.Date(yield_time)
  )

# Discover field-year IDs if not provided
ids <- read_trial_lists(base_dir)
all_ids <- intersect(ids$data_ids, ids$bdry_ids)
if (is.null(ffy_ids)) {
  ffy_ids <- all_ids
} else {
  ffy_ids <- intersect(ffy_ids, all_ids)
}

if (!length(ffy_ids)) stop("No usable ffy_ids found.")

# Optional: exclude known problematic trials (edit as needed)
bad_ids <- c("15_1_2023","15_2_2023","15_3_2023","27_1_2023","9_1_2022","9_2_2022")
ffy_ids <- setdiff(ffy_ids, bad_ids)

```

## 2. Build Per-Field Datasets (Soils → Topography → Weather)

```{r build-loop}

per_field <- vector("list", length(ffy_ids)); names(per_field) <- ffy_ids
for (i in seq_along(ffy_ids)) {
  id <- ffy_ids[[i]]
  log_message(sprintf("[%d/%d] %s", i, length(ffy_ids), id))
  per_field[[i]] <- build_field_dataset(
    ffy_id       = id,
    base_dir     = base_dir,
    date_manifest = date_manifest,
    cache_dir    = out_dir,
    elev_z       = elev_z,
    crs_out      = crs_out
  )
}
```

## 3. Stack, Validate, and Persist Outputs

```{r stack-validate-save}

# Stack attributes (geometry is not stacked here to keep a flat rectangular table)

stacked <- stack_fields(per_field)

# Minimal diagnostics
cat("\nRows:", nrow(stacked), " Cols:", ncol(stacked),"\n")
cat("CRS of first sf object:", sf::st_crs(per_field[[1]])$epsg, "\n")

# NA check for key variables (edit these names to match your schema)
key_vars <- c("yield", "n_rate", "s_rate", "clay", "sand", "silt", "water_storage",
              "elev", "slope", "aspect", "TPI",
              "precip_N_to_yield", "gdd_N_to_yield", "heavy_rain_days_postN_d15")
iss <- intersect(key_vars, names(stacked))
if (length(iss)) {
  na_rate <- sapply(stacked[, iss, drop=FALSE], function(x) mean(is.na(x)))
  print(sort(na_rate, decreasing = TRUE))
}

# Persist
save_processed(per_field, fname = "by_field_list.rds", out_dir = out_dir)
save_processed(stacked,   fname = "stacked_analysis_table.rds", out_dir = out_dir)


```

