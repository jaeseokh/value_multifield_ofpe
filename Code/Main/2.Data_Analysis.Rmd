---
title: "2.Analysis_Results(GAM)"
author: "Jaeseok Hwang"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  word_document: defaulta
---




## Knitr option

```{r, cache = F, echo = F, results = "hide"}
#####

library(knitr)

knitr::opts_chunk$set(
  cache = FALSE,
  echo = FALSE,
  warning = FALSE,
  cache.lazy = FALSE,
  fig.retina = 6,
  fig.height = 9,
  fig.width = 9,
  message = FALSE,
  error = TRUE
)

options(knitr.duplicate.label = "allow")

```


#### Packages 

```{r pacakages, cache = FALSE, results = "hide"}

library(here)
library(rmarkdown) # for rmarkdown options
library(jsonlite) # for json data loading and processing
library(parallel) # for parallel processing (computing)
library(bookdown) # for bookdown options
library(knitr) # for knitr options
library(stringr) # for string manipulation

library(measurements) # for unit conversion
library(data.table) # for data manipulation
library(tidyverse) # for data manipulation
library(dplyr) # for data manipulation

library(tmap) # for mapping
library(ggplot2) # for plotting

library(sf) # for spatial data
library(stars) # some raster data needs to be stacked in stars format
library(raster) # for raster data
library(exactextractr) # to extract the raster data from stacked star

library(terra) # to calculate the topographic variables
library(spatialEco) # to calculate the topographic variables
library(elevatr) # for the dem data ( digital eleveation model)
library(soilDB) # for soil survey data ( SSURGO)
library(FedData) # for soil survey data ( SSURGO)
library(daymetr) # for daymet data ( weather info)

###################
# Load necessary libraries

library(future)
library(future.apply)

library(mlr3verse)
library(mlr3)
library(mlr3learners)

library(xgboost)
library(ranger)
library(randomForest)
library(grf)
library(caret)
library(dplyr)
library(mgcv)
library(R6)



```

# Read sources and load processed data

```{r source, echo = F, results = "hide"}

# Read functions for data processing 
source(here("Code","Main","0_Set_up_preparation.R"))
source(here("Code","Functions","functions_for_analysis.R"))

```

```{r set up field data and info , cache = T, results = "hide"}
 
dat_binded <- readRDS(here("Data", "Processed", "Analysis_ready", "dat_binded.rds"))
info_binded <- readRDS(here("Data", "Processed", "Analysis_ready", "info_binded.rds"))
n_table <- readRDS(here("Data", "Processed", "Analysis_ready","n_table_anony.RDS"))

base_ffy <-n_table$ffy_id[which(n_table$N_base !=0)]

dat_nozero <- dat_binded %>% filter( ffy_id  %in% base_ffy )  %>%
           dplyr::select(yield,n_rate,elev,slope,aspect,tpi,clay,sand
           ,silt,water_storage,prcp_t,gdd_t,edd_t,ffy_id)  # 72 field-year data

dat_nozero <- na.omit(dat_nozero)

#  Use GAM (Generalized Additive Model) to model within-field responses.

# Define relevant columns
reg_col <- c("yield", "n_rate", "elev", "slope","aspect", "clay", 
        "sand", "silt", "water_storage", "prcp_t", "gdd_t","edd_t")

```

```{r xgboost , cache = T, results = "hide"}
 
# Initialize lists to store models and predictions
xgb_model_list <- list()
xgb_preds_list <- list()

# Define parallel backend
plan(multisession, workers = parallel::detectCores() - 1)

# Function to train and predict for one fold
process_xgb <- function(i) {
  # Split the data into train and test sets
  test_data <- dat_nozero[ffy_id == base_ffy[i], ..reg_col]
  train_data <- dat_nozero[ffy_id != base_ffy[i],..reg_col]
  
  # Remove rows with missing values
  train_data <- na.omit(train_data)
  test_data <- na.omit(test_data)
  
  # Prepare training and test matrices
  x_train <- train_data[, !"yield", with = FALSE]
  y_train <- train_data$yield
  x_test <- test_data[, !"yield", with = FALSE]
  
  # Split the training data into train and validation sets
  set.seed(123)
  train_index <- createDataPartition(y_train, p = 0.8, list = FALSE)
  x_train_subset <- x_train[train_index, ]
  y_train_subset <- y_train[train_index]
  x_valid <- x_train[-train_index, ]
  y_valid <- y_train[-train_index]
  
  # Convert data to DMatrix format
  dtrain <- xgb.DMatrix(data = as.matrix(x_train_subset), label = y_train_subset)
  dvalid <- xgb.DMatrix(data = as.matrix(x_valid), label = y_valid)
  dtest <- xgb.DMatrix(data = as.matrix(x_test))
  
  # Hyperparameters for XGBoost
  params <- list(
    objective = "reg:squarederror",
    eval_metric = "rmse",
    max_depth = 6,
    eta = 0.1,
    subsample = 0.8,
    colsample_bytree = 0.8,
    nthread =1
  )
  
  # Train the XGBoost model with early stopping
  watchlist <- list(train = dtrain, eval = dvalid)
  xgb_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = 200,
    watchlist = watchlist,
    early_stopping_rounds = 10,
    print_every_n = 10,
    verbose = 0
  )
  
  # Make predictions
  xgb_preds <- predict(xgb_model, dtest)
  
  # Return the model and predictions
  list(model = xgb_model, predictions = xgb_preds)
}

# Run the process in parallel
start_time <- Sys.time()
plan(sequential) 
results <- future_lapply(1:length(base_ffy), process_xgb)
end_time <- Sys.time()

# Extract models and predictions
xgb_model_list <- lapply(results, function(x) x$model)
xgb_preds_list <- lapply(results, function(x) x$predictions)

# Save results
# saveRDS(xgb_model_list, here("Data", "Processed", "Analysis_results", "xgb_model_list.rds"))
# saveRDS(xgb_preds_list, here("Data", "Processed", "Analysis_results", "xgb_preds_list.rds"))

 saveRDS(xgb_model_list, here("Data", "Processed", "Analysis_results", "xgb_model_list_nozero.rds"))
 saveRDS(xgb_preds_list, here("Data", "Processed", "Analysis_results", "xgb_preds_list_nozero.rds"))


# Reset the parallel backend
plan(sequential)

print(end_time - start_time)


```

```{r random forest , cache = T, results = "hide"}
 
# Define the predictor columns
reg_col <- c("yield", "n_rate", "elev", "slope", "aspect", "clay", 
                 "sand", "silt", "water_storage", "prcp_t", "gdd_t", "edd_t")

# Ensure dat_nozero is a data.table and remove NA values

setDT(dat_nozero)
dat_nozero <- na.omit(dat_nozero)

# Initialize list to store results
rf_results <- vector("list", length(base_ffy))

# Define the function
process_rf <- function(i, dat_nozero, base_ffy, reg_col) {
  # Ensure ffy_id exists in the dataset
  if (!"ffy_id" %in% names(dat_nozero)) {
    stop("ffy_id column not found in dat_nozero")
  }

  # Select train and test data correctly
  test_data <- dat_nozero[ffy_id == base_ffy[i], reg_col, with = FALSE]
  train_data <- dat_nozero[ffy_id != base_ffy[i], reg_col, with = FALSE]

  # Train the Random Forest model using ranger
  rf_model <- ranger(
    formula = yield ~ ., 
    data = train_data, 
    num.trees = 200, 
    mtry = as.integer(log2(ncol(train_data) - 1)), 
    min.node.size = 5, 
    sample.fraction = 0.7, 
    importance = "impurity", 
    replace = TRUE, 
    max.depth = 10
  )

  # Make predictions on the test set
  y_pred <- predict(rf_model, test_data)$predictions

  # Return the model and predictions
  list(model = rf_model, predictions = y_pred)
}

# Run in a standard for loop (SEQUENTIAL)
start_time <- Sys.time()
for (i in seq_along(base_ffy)) {
  rf_results[[i]] <- process_rf(i, dat_nozero, base_ffy, reg_columns)
}

end_time <- Sys.time()

# Print execution time
print(end_time - start_time)

# Extract models and predictions
rf_model_list <- lapply(rf_results, function(x) x$model)
rf_pred_list <- lapply(rf_results, function(x) x$predictions)

# Save results
saveRDS(rf_model_list, here("Data", "Processed", "Analysis_results", "rf_model_list_nozero.rds"))
saveRDS(rf_pred_list, here("Data", "Processed", "Analysis_results", "rf_pred_list_nozero.rds"))

# Reset the parallel backend
plan(sequential)
```

```{r causal forest  , cache = T, results = "hide"}
 
# Define batch size
batch_size <- 10

# Calculate the total number of batches
num_batches <- ceiling(72/ batch_size)

# Initialize lists to store intermediate results
cf_model_list <- list()
cf_pred_list <- list()

# Loop through batches
for (batch in 1:num_batches) {
  # Define the range of indices for this batch
  start_idx <- (batch - 1) * batch_size + 1
  end_idx <- min(batch * batch_size, length(base_ffy))  # Ensure it doesn't exceed the total
  
  # Temporary lists for this batch
  cf_model_list_batch <- list()
  cf_pred_list_batch <- list()
  
  for (i in start_idx:end_idx) {
    # Filter test and train datasets
    test_data <- dat_nozero[ffy_id == base_ffy[i], ..reg_col]
    train_data <- dat_nozero[ffy_id != base_ffy[i], ..reg_col]
    
    # Remove rows with missing values
    train_data <- na.omit(train_data)
    test_data <- na.omit(test_data)
    
    # Check if sufficient data is available for training
    if (nrow(train_data) == 0 || nrow(test_data) == 0) {
      warning(paste("Insufficient data for ffy_id:", base_ffy[i]))
      next
    }
    
    # Prepare training and test matrices
    x_train <- train_data[, !c("yield"), with = FALSE]
    y_train <- train_data$yield
    x_test <- test_data[, !c("yield"), with = FALSE]
    treatment <- train_data$n_rate
    
    # Train the causal forest model
    cf_model <- causal_forest(
      X = as.matrix(x_train),
      Y = y_train,
      W = treatment,
      num.trees = 200,
      min.node.size = 5,
      sample.fraction = 0.5
    )
    
    # Make predictions on test data
    cf_pred <- predict(cf_model, newdata = as.matrix(x_test))$predictions
    
    # Adjust predictions to represent actual yield levels
    adjusted_cf_pred <- cf_pred + mean(y_train, na.rm = TRUE)
    
    # Save results for this fold in the batch
    cf_model_list_batch[[i]] <- cf_model
    cf_pred_list_batch[[i]] <- adjusted_cf_pred
    
    # Print progress
    cat("Completed fold", i, "of", length(base_ffy), "\n")
  }
  
  # Save each batch as a separate RDS file
  saveRDS(cf_model_list_batch, here("Data", "Processed", "Analysis_results", paste0("cf_model_list_batch_", batch, ".rds")))
  saveRDS(cf_pred_list_batch, here("Data", "Processed", "Analysis_results", paste0("cf_pred_list_batch_", batch, ".rds")))
  
  # Clear memory for the next batch
  cf_model_list_batch <- list()
  cf_pred_list_batch <- list()
  gc()  # Run garbage collection to free memory
}

cat("All batches completed.\n")

# Reset the parallel backend
plan(sequential)



# Locate all cf_pred_list batch files
batch_files <- list.files(
  path = here("Data", "Processed", "Analysis_results"),
  pattern = "cf_pred_list_batch_.*\\.rds$",
  full.names = TRUE
)

# Load all cf_pred_list batch files into a list
cf_pred_list_batches <- lapply(batch_files, readRDS)

# Combine all batches into a single list
combined_cf_pred_list <- do.call(c, cf_pred_list_batches)

# Remove empty or NULL elements
combined_cf_pred_list <- combined_cf_pred_list[!sapply(combined_cf_pred_list, is.null)]


# Locate all cf_model_list batch files
model_batch_files <- list.files(
  path = here("Data", "Processed", "Analysis_results"),
  pattern = "cf_model_list_batch_.*\\.rds$",
  full.names = TRUE
)

# Load all cf_model_list batch files into a list
cf_model_list_batches <- lapply(model_batch_files, readRDS)

# Combine all batches into a single list
combined_cf_model_list <- do.call(c, cf_model_list_batches)

# Remove empty or NULL elements
combined_cf_model_list <- combined_cf_model_list[!sapply(combined_cf_model_list, is.null)]


# Save the combined list (optional)
saveRDS(combined_cf_pred_list, here("Data", "Processed", "Analysis_results", "cf_pred_list.rds"))

# Save the combined list
saveRDS(combined_cf_model_list, here("Data", "Processed", "Analysis_results", "cf_model_list_combined.rds"))

# Verify the combined list
cat("Number of elements in combined cf_model_list:", length(combined_cf_model_list), "\n")

```


```{r check var importance , cache = T, results = "hide"}

xgb_model_list <- readRDS(here("Data", "Processed", "Analysis_results", "xgb_model_list_nozero.rds"))
xgb_pred_list <- readRDS(here("Data", "Processed", "Analysis_results", "xgb_preds_list_nozero.rds"))

rf_model_list <- readRDS(here("Data", "Processed", "Analysis_results", "rf_model_list_nozero.rds"))
rf_pred_list  <- readRDS(here("Data", "Processed", "Analysis_results", "rf_pred_list_nozero.rds"))

cf_pred_list <- readRDS(here("Data", "Processed", "Analysis_results", "cf_pred_list.rds"))
combined_cf_model_list <- readRDS(here("Data", "Processed", "Analysis_results", "cf_model_list_combined.rds"))


# Extract and rank variable importance from XGBoost models
xgb_rank_tab <- bind_rows(lapply(seq_along(xgb_model_list), function(i) {
  xgb.importance(model = xgb_model_list[[i]]) %>%
    arrange(desc(Gain)) %>%
    pull(Feature) %>%
    as.data.frame() %>%
    t() %>%
    as.data.frame()
})) %>% setDT()

# Convert wide table to long format and count occurrences of each feature per rank
xgb_rank_counts <- melt(xgb_rank_tab, variable.name = "Rank", value.name = "Feature") %>%
  count(Rank, Feature) %>%
  pivot_wider(names_from = Rank, values_from = n, values_fill = 0)

# ✅ Rename columns with numeric ranks (1, 2, 3, ..., 12)
colnames(xgb_rank_counts) <- c("Variable", as.character(1:12))

# Convert to long format and sort
xgb_rank_list <- pivot_longer(xgb_rank_counts, cols = -Variable, names_to = "Rank", values_to = "Count") %>%
  mutate(Rank = as.numeric(Rank)) %>%  # ✅ Convert Rank to numeric
  filter(Count > 0) %>%
  arrange(Rank, desc(Count))

# Split data into sub-tables (10 rows each)
xgb_sub_tables <- split(xgb_rank_list, ceiling(seq_len(nrow(xgb_rank_list)) / 10))

# Create Flextable list
xgb_flextable_list <- lapply(seq_along(xgb_sub_tables), function(i) {
  flextable(sub_tables[[i]]) %>%
    theme_vanilla() %>%
    autofit() %>%
    set_caption(paste0("XGBoost: Ranked Feature Importance (Part ", i, " of ", length(sub_tables), ")"))
})

xgb_flextable_list


# Extract and rank variable importance from Random Forest models
rf_rank_tab <- bind_rows(lapply(seq_along(rf_model_list), function(i) {
  rf_model_list[[i]]$variable.importance %>%
    as.data.frame() %>%
    rownames_to_column(var = "Feature") %>%
    rename(Gain = ".") %>%
    arrange(desc(Gain)) %>%
    pull(Feature) %>%
    t() %>%
    as.data.frame()
})) 

# Ensure rf_rank_tab is a data.table
setDT(rf_rank_tab)

# Rename columns as Rank_1, Rank_2, Rank_3, ..., Rank_n
setnames(rf_rank_tab, old = names(rf_rank_tab), new = paste0("Rank_", seq_along(names(rf_rank_tab))))

# Convert wide table to long format and count occurrences of each feature per rank
rf_long <- melt(rf_rank_tab, measure.vars = names(rf_rank_tab), variable.name = "Rank", value.name = "Feature")

# ✅ Fix Rank naming to remove "Rank_" prefix for clean table
rf_long[, Rank := as.numeric(gsub("Rank_", "", Rank))]

# Count occurrences of each feature per rank
rf_rank_counts <- rf_long[, .N, by = .(Rank, Feature)] %>%
  dcast(Feature ~ Rank, value.var = "N", fill = 0)

# ✅ Rename columns to simple numeric format (1, 2, 3, ..., 12)
setnames(rf_rank_counts, old = names(rf_rank_counts)[-1], new = as.character(seq_along(names(rf_rank_counts)[-1])))

# Convert to long format and sort
rf_rank_list <- melt(rf_rank_counts, id.vars = "Feature", variable.name = "Rank", value.name = "Count") %>%
  mutate(Rank = as.numeric(Rank)) %>%
  filter(Count > 0) %>%
  arrange(Rank, desc(Count))

# Split data into sub-tables (10 rows each)
rf_sub_tables <- split(rf_rank_list, ceiling(seq_len(nrow(rf_rank_list)) / 10))

# Create Flextable list
rf_flextable_list <- lapply(seq_along(rf_sub_tables), function(i) {
  flextable(rf_sub_tables[[i]]) %>%
    theme_vanilla() %>%
    autofit() %>%
    set_caption(paste0("Random Forest: Ranked Feature Importance (Part ", i, " of ", length(rf_sub_tables), ")"))
})

# Display all tables
rf_flextable_list



library(webshot)

# Ensure webshot is installed and initialized (only run once)
webshot::install_phantomjs()

# Function to save flextables as PNG
save_flextable_as_image <- function(ft, filename) {
  temp_file <- tempfile(fileext = ".png")
  save_as_image(ft, path = temp_file)
  file.rename(temp_file, filename)
}

# Save XGBoost tables
save_flextable_as_image(xgb_flextable_list[[1]], here("Results","Tables","xgb_table_1.png"))
save_flextable_as_image(xgb_flextable_list[[2]], here("Results","Tables","xgb_table_2.png"))

# Save Random Forest tables
save_flextable_as_image(rf_flextable_list[[1]], here("Results","Tables","rf_table_1.png"))
save_flextable_as_image(rf_flextable_list[[2]], here("Results","Tables","rf_table_2.png"))



```



```{r estimate eonr of field i , cache = T, results = "hide"}

field_reg_vars <- c("elev", "slope", "aspect", "clay", 
                 "sand", "silt", "water_storage", "prcp_t", "gdd_t", "edd_t")

 # Make formula for gam regression with direct OFPE data for field i
   formula_gam <- paste0(
    "yield ~ s(n_rate, k = 3) ", 
    paste0(" + ", paste0(field_reg_vars, collapse = " + "))
  ) %>% formula()



result_tab_list <- list()

for(i in 1:length(base_ffy)){
 
test_data <- dat_nozero[ffy_id == base_ffy[i],]
test_data <-  na.omit(test_data)

# Skip iteration if test_data is empty
  if (nrow(test_data) == 0) {
    warning(paste("No data for ffy_id:", base_ffy[i]))
    next
  }
  
  # Check if predictions are valid for xgb and rf
  if (is.null(xgb_pred_list[[i]]) || anyNA(xgb_pred_list[[i]])) {
    warning(paste("Invalid or missing predictions for XGB at ffy_id:", base_ffy[i]))
    next
  }
  if (is.null(rf_pred_list[[i]]) || anyNA(rf_pred_list[[i]])) {
    warning(paste("Invalid or missing predictions for RF at ffy_id:", base_ffy[i]))
    next
  }

  
  xgb_gam_data <- test_data %>% mutate(yield = xgb_pred_list[[i]])
  rf_gam_data <-  test_data %>% mutate(yield = rf_pred_list[[i]])
  cf_gam_data <-  test_data %>% mutate(yield = cf_pred_list[[i]])
  
  
  gam_ofpe <- gam(formula_gam, data = test_data)
  gam_xgb <- gam(formula_gam, data = xgb_gam_data)
  gam_rf <- gam(formula_gam, data = rf_gam_data)
  gam_cf <- gam(formula_gam, data = cf_gam_data)

  # Calculate mean values for other variables
    gam_for_eval <- test_data[, lapply(.SD, mean, na.rm = TRUE), .SDcols = field_reg_vars]

   # Generate 100 rows for eval_data with n_rate ranging from min to max
   n_rate_seq <- seq(min(test_data$n_rate, na.rm = TRUE), max(test_data$n_rate, na.rm = TRUE), length.out = 100)

   eval_data <- data.table(n_rate = n_rate_seq)[
          ,cbind(.SD, gam_for_eval[rep(1, .N), ]), .SDcols = "n_rate"]

# Predict yield using the GAM models
  ofpe_yield <- predict(gam_ofpe, newdata = eval_data)
  xgb_yield <- predict(gam_xgb, newdata = eval_data)
  rf_yield <- predict(gam_rf, newdata = eval_data)
 cf_yield <- predict(gam_cf, newdata = eval_data)

  eval_comb <- data.frame(
  ofpe = ofpe_yield,
  xgb = xgb_yield,
  rf = rf_yield,
  cf = cf_yield,
  n_rate = n_rate_seq
)
   
 # Calculate EONR and maximum profit
 ### Create a temporary eval_comb data.table for profit calculations  

eonr_tab <- price_tab %>% data.table %>%
  .[, {
  eval_comb_temp <- eval_comb %>% data.table %>%
  .[, `:=`(
    profit_ofpe = round(corn * ofpe - nitrogen * n_rate,1),
    profit_xgb = round(corn * xgb - nitrogen * n_rate,1),
   profit_cf = round(corn * cf - nitrogen * n_rate,1),
    profit_rf = round(corn * rf - nitrogen * n_rate,1)
  )]
  
  ### Calculate EONR and maximum profit for each method
  eval_comb_temp[, .(
    ofpe_n = round(n_rate[which.max(profit_ofpe)],1),
   ofpe_p = max(profit_ofpe, na.rm = TRUE),
    xgb_n =  round(n_rate[which.max(profit_xgb)],1),
    xgb_p = profit_ofpe[which.max(profit_xgb)],  # Profit using ofpe yield at xgb_n
    cf_n =  round(n_rate[which.max(profit_cf)],1),
    cf_p = profit_ofpe[which.max(profit_cf)],    # Profit using ofpe yield at cf_n
    rf_n =  round(n_rate[which.max(profit_rf)],1),
    rf_p = profit_ofpe[which.max(profit_rf)]  
  )]
}, by = .(year, corn, nitrogen)] ### Group by year, corn price, and nitrogen price from price_tab

# Create the result tibble
result_dat_tab <- tibble(
  ffy_id = base_ffy[i],   # Store ffy_id_dat[i] as a regular column
  eval_comb = list(eval_comb),    # Store eval_comb as a list-column
  eonr_tab = list(eonr_tab)      # Store eonr_tab as a list-column
)

result_tab_list[[i]] <- result_dat_tab

}

saveRDS(result_tab_list, here("Data", "Processed", "Analysis_results", "result_tab_list.rds"))


```

  
```{r rest of the part , cache = T, results = "hide"}

  # Generate fitted values for plotting

 gam_for_eval <- gam_fit_dat %>%
  summarise(across(everything(), mean, na.rm = TRUE))
    
 eval_data <- test_data %>%
  dplyr::select(n_rate) %>%
  bind_cols(replicate(nrow(gam_for_eval), gam_for_eval, simplify = FALSE))
  
  results_df$xgb_fitted <- predict(gam_xgb, newdata =  eval_data  )
  results_df$rf_fitted <- predict(gam_rf, newdata =  eval_data )
  results_df$cf_fitted <- predict(gam_cf, newdata =  eval_data )
   results_df$ofpe_fitted <- predict(gam_ofpe, newdata =  eval_data )
 
  # Plot the GAM fits
  ggplot(results_df, aes(x = n_rate)) +
    geom_point(aes(y = xgb_yield, color = "XGBoost")) +
    geom_point(aes(y = rf_yield, color = "Random Forest")) +
    geom_point(aes(y = cf_yield, color = "Causal Forest")) +
      geom_point(aes(y = ofpe_yield, color = "OFPE Yield")) +
  
    geom_line(aes(y = xgb_fitted, color = "XGBoost (GAM fit)")) +
    geom_line(aes(y = rf_fitted, color = "Random Forest (GAM fit)")) +
    geom_line(aes(y = cf_fitted, color = "Causal Forest (GAM fit)")) +
     geom_line(aes(y = ofpe_fitted, color = "OFPE (GAM fit)")) +
   
    labs(title = "Yield N Responses (GAM Fit)",
         x = "N Rate",
         y = "Predicted (XGB,RF,CF) and Observed (OFPE) Yield",
         color = "Model") +
    theme_minimal()
  

  return(list(
    gam_xgb = gam_xgb,
    gam_rf = gam_rf,
    gam_cf = gam_cf,
    gam_ofpe = gam_ofpe,
    plot = last_plot()
  ))
```