---
title: "2.Analysis_Results(GAM)"
author: "Jaeseok Hwang"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  word_document: defaulta
---




## Knitr option

```{r, cache = F, echo = F, results = "hide"}
#####

library(knitr)

knitr::opts_chunk$set(
  cache = FALSE,
  echo = FALSE,
  warning = FALSE,
  cache.lazy = FALSE,
  fig.retina = 6,
  fig.height = 9,
  fig.width = 9,
  message = FALSE,
  error = TRUE
)

options(knitr.duplicate.label = "allow")

```


#### Packages 

```{r pacakages, cache = FALSE, results = "hide"}

library(here)
library(rmarkdown) # for rmarkdown options
library(jsonlite) # for json data loading and processing
library(parallel) # for parallel processing (computing)
library(bookdown) # for bookdown options
library(knitr) # for knitr options
library(stringr) # for string manipulation

library(measurements) # for unit conversion
library(data.table) # for data manipulation
library(tidyverse) # for data manipulation
library(dplyr) # for data manipulation

library(tmap) # for mapping
library(ggplot2) # for plotting

library(sf) # for spatial data
library(stars) # some raster data needs to be stacked in stars format
library(raster) # for raster data
library(exactextractr) # to extract the raster data from stacked star

library(terra) # to calculate the topographic variables
library(spatialEco) # to calculate the topographic variables
library(elevatr) # for the dem data ( digital eleveation model)
library(soilDB) # for soil survey data ( SSURGO)
library(FedData) # for soil survey data ( SSURGO)
library(daymetr) # for daymet data ( weather info)

###################
# Load necessary libraries

library(future)
library(future.apply)

library(mlr3verse)
library(mlr3)
library(mlr3learners)

library(xgboost)
library(ranger)
library(randomForest)
library(grf)
library(caret)
library(dplyr)
library(mgcv)
library(R6)



```

# Read sources and load processed data

```{r source, echo = F, results = "hide"}

# Read functions for data processing 
source(here("Code","Main","0_Set_up_preparation.R"))
source(here("Code","Functions","functions_for_analysis.R"))

```




```{r data sort for combined analysis, cache = T, results = "hide"}
  
### Check all the field_year list (ffy) in the processed data(Analysis_Ready) folder

ffy_merged_dat <- list.files(here("Data","Processed","Analysis_ready")) %>%
 str_subset("_merged_data.rds") %>%
   str_remove("_merged_data.rds")


ffy_weather_info <- list.files(here("Data","Processed","Analysis_ready")) %>%
 str_subset("_weather_info.rds") %>%
   str_remove("_weather_info.rds")

ffy_id_dat <-readRDS(here("Data","Processed","ffy_id_list.rds"))


match(ffy_merged_dat, ffy_weather_info)
match(ffy_merged_dat, ffy_id_dat)

```


```{r xgboost & random forest data setup , cache = T, results = "hide"}

# Combine merged data and weather info data
info_tb_list <- list()
dat_tb_list <- list()

for(i in 1:length(ffy_merged_dat)) {
  
  ffy_id <- ffy_merged_dat[i]
  # Read Sf data and weather info table
  dat_sf <- readRDS(here("Data", "Processed", "Analysis_ready", paste0(ffy_id, "_merged_data.rds")))

  dat_sf <- dat_sf %>% st_transform(4326)
 # Drop geometry and convert to data.table
  dat_tb <- dat_sf %>% st_drop_geometry() %>% as.data.table()
  
  dat_tb$ffy_id <- ffy_id

  dat_tb[, c("farm", "field", "year") := tstrsplit(ffy_id, "_", type.convert = TRUE)]

  info_tb <- readRDS(here("Data", "Processed", "Analysis_ready", paste0(ffy_id, "_weather_info.rds")))
 
  dat_tb[, c("prcp_t", "gdd_t","edd_t","gdd_stg","edd_stg","gdd_stg_n","edd_stg_n","y_time","n_time","s_time") 
        := .(info_tb$prcp_t, info_tb$gdd_t,info_tb$edd_t,info_tb$stage_gdd_t, info_tb$stage_edd_t,
          info_tb$stage_gdd_nitrogen,info_tb$stage_edd_nitrogen,info_tb$yield_time,info_tb$n_time,info_tb$s_time)]
 
  dat_tb_list[[i]] <- dat_tb
  info_tb_list[[i]] <- info_tb
}

  dat_bind <-bind_rows(dat_tb_list)
  info_bind <- bind_rows(info_tb_list)

   saveRDS(dat_bind,here("Data", "Processed", "Analysis_ready", "dat_binded.rds"))
    saveRDS(info_bind,here("Data", "Processed", "Analysis_ready", "info_binded.rds"))


```


###### Basic statistical check-up of data
 
# Create test_data and train_data using data.table

```{r xgboost to predict field -i , cache = T, results = "hide"}
 
 dat_binded <- readRDS(here("Data", "Processed", "Analysis_ready", "dat_binded.rds"))
info_binded <- readRDS(here("Data", "Processed", "Analysis_ready", "info_binded.rds"))



# Define relevant columns
relevant_columns <- c("yield", "n_rate", "elev", "slope","aspect", "tpi", "clay", 
                      "sand", "silt", "water_storage", "prcp_t", "gdd_t","edd_t")


#1. n_range and distributino of n 

hist(dat_binded$n_rate)
hist(dat_binded$prcp_t)
hist(dat_binded$gdd_t)
hist(dat_binded$edd_t)

quantile(dat_binded$n_rate, seq(0,1,0.05))
hist(dat_binded$n_rate)

# okay let's think about the single field case
# what is the potential feature of single field OFPE data

# Function to classify and predict yield-N response for each field
# Function to classify yield-N response for a single field
classify_yield_response <- function(data) {
  # Filter for unique ffy_id
  field_data <- data
  
  # Ensure there are enough rows to fit a model
  if (nrow(field_data) < 3) {
    return(data.frame(ffy_id = unique(field_data$ffy_id), classification = NA))
  }
  
  # Fit quadratic model
  model <- lm(yield ~ n_rate + I(n_rate^2), data = field_data)
  coeffs <- coef(model)
  b1 <- coeffs[2]
  b2 <- coeffs[3]
  
  # Classify based on coefficients
  classification <- if (b2 < 0) {
    if (b1 > 0) {
      "Concave: Increases with N"
    } else {
      "Concave: Decreases with N"
    }
  } else {
    if (b1 > 0) {
      "Convex: Increases with N"
    } else {
      "Convex: Decreases with N"
    }
  }
  
  # Add classification to the original data
  field_data$classification <- classification
  return(field_data)
}

# Apply classification to all fields
classified_data <- dat_binded %>%
  group_by(ffy_id) %>%
  group_modify(~ classify_yield_response(.)) %>%
  ungroup()

# Plot yield-N response curves by response type (facet_wrap)
yield_n_by_type <- ggplot(classified_data, aes(x = n_rate, y = yield, group = ffy_id, color = ffy_id)) +
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE, size = 1) +
  facet_wrap(~ classification, scales = "free_y") +
  labs(
    title = "Yield-N Response Curves by Response Type",
    x = "Nitrogen Rate",
    y = "Yield",
    color = "Field ID"
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 10, face = "bold"),
    plot.title = element_text(size = 14, face = "bold"),
    legend.position = "bottom"
  )


```

```{r xgboost to predict field -i , cache = T, results = "hide"}
 
 dat_binded <- readRDS(here("Data", "Processed", "Analysis_ready", "dat_binded.rds"))
info_binded <- readRDS(here("Data", "Processed", "Analysis_ready", "info_binded.rds"))

# Define relevant columns
relevant_columns <- c("yield", "n_rate", "elev", "slope","aspect", "tpi", "clay", 
                      "sand", "silt", "water_storage", "prcp_t", "gdd_t","edd_t")



dat_binded <- dat_scaled

# Initialize lists to store models and predictions
xgb_model_list <- list()
xgb_preds_list <- list()

# Define parallel backend
plan(multisession, workers = parallel::detectCores() - 1)

# Function to train and predict for one fold
process_xgb <- function(i) {
  # Split the data into train and test sets
  test_data <- dat_binded[ffy_id == ffy_merged_dat[i], ..relevant_columns]
  train_data <- dat_binded[ffy_id != ffy_merged_dat[i], ..relevant_columns]
  
  # Remove rows with missing values
  train_data <- na.omit(train_data)
  test_data <- na.omit(test_data)
  
  # Prepare training and test matrices
  x_train <- train_data[, !"yield", with = FALSE]
  y_train <- train_data$yield
  x_test <- test_data[, !"yield", with = FALSE]
  
  # Split the training data into train and validation sets
  set.seed(123)
  train_index <- createDataPartition(y_train, p = 0.8, list = FALSE)
  x_train_subset <- x_train[train_index, ]
  y_train_subset <- y_train[train_index]
  x_valid <- x_train[-train_index, ]
  y_valid <- y_train[-train_index]
  
  # Convert data to DMatrix format
  dtrain <- xgb.DMatrix(data = as.matrix(x_train_subset), label = y_train_subset)
  dvalid <- xgb.DMatrix(data = as.matrix(x_valid), label = y_valid)
  dtest <- xgb.DMatrix(data = as.matrix(x_test))
  
  # Hyperparameters for XGBoost
  params <- list(
    objective = "reg:squarederror",
    eval_metric = "rmse",
    max_depth = 6,
    eta = 0.1,
    subsample = 0.8,
    colsample_bytree = 0.8
  )
  
  # Train the XGBoost model with early stopping
  watchlist <- list(train = dtrain, eval = dvalid)
  xgb_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = 500,
    watchlist = watchlist,
    early_stopping_rounds = 10,
    print_every_n = 10,
    verbose = 0
  )
  
  # Make predictions
  xgb_preds <- predict(xgb_model, dtest)
  
  # Return the model and predictions
  list(model = xgb_model, predictions = xgb_preds)
}

# Run the process in parallel
start_time <- Sys.time()
results <- future_lapply(1:length(ffy_merged_dat), process_xgb)
end_time <- Sys.time()

# Extract models and predictions
xgb_model_list <- lapply(results, function(x) x$model)
xgb_preds_list <- lapply(results, function(x) x$predictions)

# Save results
saveRDS(xgb_model_list, here("Data", "Processed", "Analysis_results", "xgb_model_list.rds"))
saveRDS(xgb_preds_list, here("Data", "Processed", "Analysis_results", "xgb_preds_list.rds"))

# Reset the parallel backend
plan(sequential)

print(end_time - start_time)


```

```{r random forest to predict field -i , cache = T, results = "hide"}
 
##### Random Forest

 dat_binded <- readRDS(here("Data", "Processed", "Analysis_ready", "dat_binded.rds"))
 info_binded <- readRDS(here("Data", "Processed", "Analysis_ready", "info_binded.rds"))

# Define parallel backend
plan(multisession, workers = parallel::detectCores() - 1)  # Use one less core than available

# Define relevant columns
relevant_columns <- c("yield", "n_rate", "elev", "slope","aspect", "tpi", "clay", 
                      "sand", "silt", "water_storage", "prcp_t", "gdd_t")

# Function to train and predict for a single fold
process_fold <- function(i) {
  test_data <- dat_binded[ffy_id == ffy_merged_dat[i], ..relevant_columns]
  train_data <- dat_binded[ffy_id != ffy_merged_dat[i], ..relevant_columns]
  
  # Remove rows with missing values
  train_data <- na.omit(train_data)
  test_data <- na.omit(test_data)
  
  # Define the regression task
  task <- TaskRegr$new(id = paste0("task_", i), backend = train_data, target = "yield")
  
  # Create the Random Forest learner with updated parameters
  learner <- lrn("regr.ranger", 
                 num.trees = 200, 
                 mtry = as.integer(log2(ncol(train_data) - 1)), 
                 min.node.size = 5, 
                 sample.fraction = 0.7, 
                 importance = "impurity", 
                 replace = TRUE, 
                 max.depth = 10)
  
  # Train the learner
  learner$train(task)
  
  # Make predictions on the test set
  predictions <- learner$predict_newdata(test_data)
  
  # Extract predicted values
  y_pred <- predictions$response
  
  # Return the model and predictions
  list(model = learner, predictions = y_pred)
}

# Parallelize the loop
start_time <- Sys.time()
rf_results <- future_lapply(1:length(ffy_merged_dat), process_fold)
end_time <- Sys.time()

print(end_time - start_time)

# Extract models and predictions
rf_model_list <- lapply(rf_results, function(x) x$model)
rf_pred_list <- lapply(rf_results, function(x) x$predictions)

# Save results
saveRDS(rf_model_list, here("Data", "Processed", "Analysis_results", "rf_model_list.rds"))
saveRDS(rf_pred_list, here("Data", "Processed", "Analysis_results", "rf_pred_list.rds"))

# Reset the parallel backend
plan(sequential)



```


```{r causal forest to predict field -i , cache = T, results = "hide"}
 
##### Causal Forest

# Load data

dat_binded <- readRDS(here("Data", "Processed", "Analysis_ready", "dat_binded.rds"))
info_binded <- readRDS(here("Data", "Processed", "Analysis_ready", "info_binded.rds"))

# Define relevant columns
relevant_columns <- c("yield", "n_rate", "elev", "slope", "tpi", "clay", 
                      "sand", "silt", "water_storage", "prcp_t", "gdd_t")


# Define batch size
batch_size <- 10

# Calculate the total number of batches
num_batches <- ceiling(78/ batch_size)

# Initialize lists to store intermediate results
cf_model_list <- list()
cf_pred_list <- list()

# Loop through batches
for (batch in 1:num_batches) {
  # Define the range of indices for this batch
  start_idx <- (batch - 1) * batch_size + 1
  end_idx <- min(batch * batch_size, length(ffy_merged_dat))  # Ensure it doesn't exceed the total
  
  # Temporary lists for this batch
  cf_model_list_batch <- list()
  cf_pred_list_batch <- list()
  
  for (i in start_idx:end_idx) {
    # Filter test and train datasets
    test_data <- dat_binded[ffy_id == ffy_merged_dat[i], ..relevant_columns]
    train_data <- dat_binded[ffy_id != ffy_merged_dat[i], ..relevant_columns]
    
    # Remove rows with missing values
    train_data <- na.omit(train_data)
    test_data <- na.omit(test_data)
    
    # Check if sufficient data is available for training
    if (nrow(train_data) == 0 || nrow(test_data) == 0) {
      warning(paste("Insufficient data for ffy_id:", ffy_merged_dat[i]))
      next
    }
    
    # Prepare training and test matrices
    x_train <- train_data[, !c("yield"), with = FALSE]
    y_train <- train_data$yield
    x_test <- test_data[, !c("yield"), with = FALSE]
    treatment <- train_data$n_rate
    
    # Train the causal forest model
    cf_model <- causal_forest(
      X = as.matrix(x_train),
      Y = y_train,
      W = treatment,
      num.trees = 200,
      min.node.size = 5,
      sample.fraction = 0.5
    )
    
    # Make predictions on test data
    cf_pred <- predict(cf_model, newdata = as.matrix(x_test))$predictions
    
    # Adjust predictions to represent actual yield levels
    adjusted_cf_pred <- cf_pred + mean(y_train, na.rm = TRUE)
    
    # Save results for this fold in the batch
    cf_model_list_batch[[i]] <- cf_model
    cf_pred_list_batch[[i]] <- adjusted_cf_pred
    
    # Print progress
    cat("Completed fold", i, "of", length(ffy_merged_dat), "\n")
  }
  
  # Save each batch as a separate RDS file
  saveRDS(cf_model_list_batch, here("Data", "Processed", "Analysis_results", paste0("cf_model_list_batch_", batch, ".rds")))
  saveRDS(cf_pred_list_batch, here("Data", "Processed", "Analysis_results", paste0("cf_pred_list_batch_", batch, ".rds")))
  
  # Clear memory for the next batch
  cf_model_list_batch <- list()
  cf_pred_list_batch <- list()
  gc()  # Run garbage collection to free memory
}

cat("All batches completed.\n")

# Reset the parallel backend
plan(sequential)



# Locate all cf_pred_list batch files
batch_files <- list.files(
  path = here("Data", "Processed", "Analysis_results"),
  pattern = "cf_pred_list_batch_.*\\.rds$",
  full.names = TRUE
)

# Load all cf_pred_list batch files into a list
cf_pred_list_batches <- lapply(batch_files, readRDS)

# Combine all batches into a single list
combined_cf_pred_list <- do.call(c, cf_pred_list_batches)

# Remove empty or NULL elements
combined_cf_pred_list <- combined_cf_pred_list[!sapply(combined_cf_pred_list, is.null)]

# Save the combined list (optional)
saveRDS(combined_cf_pred_list, here("Data", "Processed", "Analysis_results", "cf_pred_list.rds"))




# Locate all cf_model_list batch files
model_batch_files <- list.files(
  path = here("Data", "Processed", "Analysis_results"),
  pattern = "cf_model_list_batch_.*\\.rds$",
  full.names = TRUE
)

# Load all cf_model_list batch files into a list
cf_model_list_batches <- lapply(model_batch_files, readRDS)

# Combine all batches into a single list
combined_cf_model_list <- do.call(c, cf_model_list_batches)

# Remove empty or NULL elements
combined_cf_model_list <- combined_cf_model_list[!sapply(combined_cf_model_list, is.null)]

# Save the combined list
saveRDS(combined_cf_model_list, here("Data", "Processed", "Analysis_results", "cf_model_list_combined.rds"))

# Verify the combined list
cat("Number of elements in combined cf_model_list:", length(combined_cf_model_list), "\n")



```






```{r estimate eonr of field i , cache = T, results = "hide"}


  # Ensure relevant columns are present
  relevant_columns <- c("yield", "n_rate", "elev", "slope","aspect","tpi", "clay", 
                        "sand", "silt", "water_storage","prcp_t","gdd_t" )

  # Make formula for gam regression with direct OFPE data for field i
   formula_gam<- paste0(
    "yield ~ s(n_rate, k = 3) ", 
    paste0(" + ", paste0(field_reg_vars, collapse = " + "))
  ) %>% formula()


 dat_binded <- readRDS(here("Data", "Processed", "Analysis_ready", "dat_binded.rds"))
 info_binded <- readRDS(here("Data", "Processed", "Analysis_ready", "info_binded.rds"))

 xgb_model_list <- readRDS(here("Data","Processed","Analysis_results","xgb_model_list.rds"))
 xgb_pred_list <- readRDS(here("Data","Processed","Analysis_results","xgb_preds_list.rds"))


#### Let's try to grasp the specialty of the dat_binded data
# how I can utilize the yield_time, s_time, n_time 


# Create the histogram of n_rate by 10
ggplot(dat_binded, aes(x = n_rate)) +
  geom_histogram(binwidth = 10, color = "black", fill = "lightblue") +
  labs(title = "Histogram of Nitrogen Rate (n_rate)",
       x = "Nitrogen Rate (n_rate)",
       y = "Frequency") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())


# Step 1: Calculate min and max n_rate for each ffy_id and check the condition
ffy_id_condition_met <- dat_binded %>%
  group_by(ffy_id) %>%
  summarize(
    min_n_rate = min(n_rate),
    max_n_rate = max(n_rate),
    satisfies_condition = min_n_rate <= 175 & max_n_rate >= 225
  ) %>%
  filter(satisfies_condition == TRUE)

# Step 2: Get the ffy_id that satisfy the condition
ffy_ids_to_keep <- ffy_id_condition_met$ffy_id

# Step 3: Filter dat_binded to keep only the rows with ffy_id that satisfy the condition
dat_scaled <- dat_binded %>%
  filter(ffy_id %in% ffy_ids_to_keep) %>%
  filter(n_rate >= 175 &  n_rate <= 225) %>%
  filter(yield  >100 & yield < 300)
 
hist(dat_scaled$n_rate)
length(unique(dat_scaled$ffy_id))
dim(dat_scaled)





# Placeholder list for variable importance rankings
var_importance_list <- list()

# Iterate through the list of XGBoost models
for (i in seq_along(xgb_model_list)) {
  # Extract variable importance
  var_importance <- xgb.importance(model = xgb_model_list[[i]])
  
  # Rank variables based on "Gain"
  ranked_vars <- var_importance %>%
    arrange(desc(Gain)) %>%
    pull(Feature)  # Extract only the variable names in ranked order
  
  # Store the ranked variables with field name
  var_importance_list[[i]] <- data.frame(Field = paste0("Field", i), t(ranked_vars))
}

# Combine all field-level rankings into a single data frame
ranked_table <- bind_rows(var_importance_list)

# Set field names as row names
row.names(ranked_table) <- ranked_table$Field
ranked_table <- ranked_table[, -1]  # Remove the Field column from the data frame

# View the final table
print(ranked_table)

library(flextable)
var_imp_tab <-ranked_table %>% flextable()



rf_model_list <- readRDS(here("Data", "Processed","Analysis_results","rf_model_list_nov18.rds"))
rf_pred_list <- readRDS(here("Data", "Processed", "Analysis_results","rf_pred_list_nov18.rds"))

cf_model_list <- readRDS(here("Data", "Processed","Analysis_results","cf_model_list.rds"))
cf_pred_list <- readRDS(here("Data", "Processed", "Analysis_results","cf_pred_list.rds"))



result_tab_list <- list()

for(i in 1:length(ffy_merged_dat)){
 
test_data <- dat_binded[ffy_id == ffy_merged_dat[i],]
test_data <-  na.omit(test_data[, ..relevant_columns])

# Skip iteration if test_data is empty
  if (nrow(test_data) == 0) {
    warning(paste("No data for ffy_id:", ffy_merged_dat[i]))
    next
  }
  
  # Check if predictions are valid for xgb and rf
  if (is.null(xgb_pred_list[[i]]) || anyNA(xgb_pred_list[[i]])) {
    warning(paste("Invalid or missing predictions for XGB at ffy_id:", ffy_merged_dat[i]))
    next
  }
  if (is.null(rf_pred_list[[i]]) || anyNA(rf_pred_list[[i]])) {
    warning(paste("Invalid or missing predictions for RF at ffy_id:", ffy_merged_dat[i]))
    next
  }

  
  
  xgb_gam_data <- test_data%>% mutate(yield = xgb_pred_list[[i]])
  rf_gam_data <-  test_data%>% mutate(yield = rf_pred_list[[i]])
  cf_gam_data <-  test_data%>% mutate(yield = cf_pred_list[[i]])
  
  
  gam_ofpe <- gam(formula_gam, data = test_data)
  gam_xgb <- gam(formula_gam, data = xgb_gam_data)
  gam_rf <- gam(formula_gam, data = rf_gam_data)
  gam_cf <- gam(formula_gam, data = cf_gam_data)

  # Calculate mean values for other variables
    gam_for_eval <- test_data[, lapply(.SD, mean, na.rm = TRUE), .SDcols = field_reg_vars]

   # Generate 100 rows for eval_data with n_rate ranging from min to max
   n_rate_seq <- seq(min(test_data$n_rate, na.rm = TRUE), max(test_data$n_rate, na.rm = TRUE), length.out = 100)

   eval_data <- data.table(n_rate = n_rate_seq)[
          ,cbind(.SD, gam_for_eval[rep(1, .N), ]), .SDcols = "n_rate"]

# Predict yield using the GAM models
  ofpe_yield <- predict(gam_ofpe, newdata = eval_data)
  xgb_yield <- predict(gam_xgb, newdata = eval_data)
  rf_yield <- predict(gam_rf, newdata = eval_data)
 cf_yield <- predict(gam_cf, newdata = eval_data)

  eval_comb <- data.frame(
  ofpe = ofpe_yield,
  xgb = xgb_yield,
  rf = rf_yield,
  cf = cf_yield,
  n_rate = n_rate_seq
)
   
 # Calculate EONR and maximum profit
 ### Create a temporary eval_comb data.table for profit calculations  

eonr_tab <- price_tab %>% data.table %>%
  .[, {
  eval_comb_temp <- eval_comb %>% data.table %>%
  .[, `:=`(
    profit_ofpe = round(corn * ofpe - nitrogen * n_rate,1),
    profit_xgb = round(corn * xgb - nitrogen * n_rate,1),
   profit_cf = round(corn * cf - nitrogen * n_rate,1),
    profit_rf = round(corn * rf - nitrogen * n_rate,1)
  )]
  
  ### Calculate EONR and maximum profit for each method
  eval_comb_temp[, .(
    ofpe_n = round(n_rate[which.max(profit_ofpe)],1),
   ofpe_p = max(profit_ofpe, na.rm = TRUE),
    xgb_n =  round(n_rate[which.max(profit_xgb)],1),
    xgb_p = profit_ofpe[which.max(profit_xgb)],  # Profit using ofpe yield at xgb_n
    cf_n =  round(n_rate[which.max(profit_cf)],1),
    cf_p = profit_ofpe[which.max(profit_cf)],    # Profit using ofpe yield at cf_n
    rf_n =  round(n_rate[which.max(profit_rf)],1),
    rf_p = profit_ofpe[which.max(profit_rf)]  
  )]
}, by = .(year, corn, nitrogen)] ### Group by year, corn price, and nitrogen price from price_tab

# Create the result tibble
result_dat_tab <- tibble(
  ffy_id = ffy_id_dat[i],   # Store ffy_id_dat[i] as a regular column
  eval_comb = list(eval_comb),    # Store eval_comb as a list-column
  eonr_tab = list(eonr_tab)      # Store eonr_tab as a list-column
)

result_tab_list[[i]] <- result_dat_tab

}

saveRDS(result_tab_list, here("Data", "Processed", "Analysis_results", "result_tab_list.rds"))


```

  
```{r rest of the part , cache = T, results = "hide"}

  # Generate fitted values for plotting

 gam_for_eval <- gam_fit_dat %>%
  summarise(across(everything(), mean, na.rm = TRUE))
    
 eval_data <- test_data %>%
  dplyr::select(n_rate) %>%
  bind_cols(replicate(nrow(gam_for_eval), gam_for_eval, simplify = FALSE))
  
  results_df$xgb_fitted <- predict(gam_xgb, newdata =  eval_data  )
  results_df$rf_fitted <- predict(gam_rf, newdata =  eval_data )
  results_df$cf_fitted <- predict(gam_cf, newdata =  eval_data )
   results_df$ofpe_fitted <- predict(gam_ofpe, newdata =  eval_data )
 
  # Plot the GAM fits
  ggplot(results_df, aes(x = n_rate)) +
    geom_point(aes(y = xgb_yield, color = "XGBoost")) +
    geom_point(aes(y = rf_yield, color = "Random Forest")) +
    geom_point(aes(y = cf_yield, color = "Causal Forest")) +
      geom_point(aes(y = ofpe_yield, color = "OFPE Yield")) +
  
    geom_line(aes(y = xgb_fitted, color = "XGBoost (GAM fit)")) +
    geom_line(aes(y = rf_fitted, color = "Random Forest (GAM fit)")) +
    geom_line(aes(y = cf_fitted, color = "Causal Forest (GAM fit)")) +
     geom_line(aes(y = ofpe_fitted, color = "OFPE (GAM fit)")) +
   
    labs(title = "Yield N Responses (GAM Fit)",
         x = "N Rate",
         y = "Predicted (XGB,RF,CF) and Observed (OFPE) Yield",
         color = "Model") +
    theme_minimal()
  

  return(list(
    gam_xgb = gam_xgb,
    gam_rf = gam_rf,
    gam_cf = gam_cf,
    gam_ofpe = gam_ofpe,
    plot = last_plot()
  ))
```